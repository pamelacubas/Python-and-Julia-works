In this essay, the article “Statistical Challenges in Online Controlled Experiments: A Review of A/B Testing Methodology” 
is analyzed. The research question of the article involves exploring bias in standard ATE estimators induced by triggering,
defining the randomization unit, aggregating data into sessions, and exploring noncompliance and other concepts from the 
causal inference literature with respect to triggering. 

Furthermore, there are some strengths and weaknesses of the document's approach to answering that question. The document 
provides a comprehensive review of the challenges and methodologies related to online experimentation, particularly
focusing on issues such as heterogeneous treatment effects, long-term effects, optional stopping, and interference. 
Strengths of the document's approach include its thorough examination of statistical methodologies and its call for 
greater collaboration between industry and academia to address research challenges in online experimentation.

However, a weakness of the document's approach is that it acknowledges the sparse literature on triggering in online 
experiments, leaving many areas open for further research . This limitation suggests that there may be gaps in the 
current understanding of bias induced by triggering and the methodologies for estimating treatment effects in such 
scenarios. Furthermore, the document highlights the challenges of violations of the SUTVA assumption and the misuse 
of hypothesis tests in online experiments, indicating ongoing practical and methodological concerns in the field. 
This recognition of potential issues in the application of statistical methods in online experimentation adds depth 
to the discussion but also underscores the need for further research and refinement in this area.

Also, the paper advances knowledge about the research question by providing a cohesive review of the statistical 
methodologies and challenges related to online experimentation, particularly focusing on sensitivity, effect size, 
heterogeneity, long-term effects, optional stopping, and interference . This review serves to introduce academicians
to the context and goals of online experimentation, highlighting the need for collaboration between industry and 
academia to address research challenges in this area .

Finally, the next steps to advance the research question involve further exploration of bias induced by triggering 
in online experiments. This includes investigating different types of bias, defining the randomization unit, 
exploring noncompliance, and considering concepts from the causal inference literature with respect to triggering.
Additionally, collaboration between academia and industry practitioners in the field of online experimentation is
crucial to address the challenges and open problems in this area.
